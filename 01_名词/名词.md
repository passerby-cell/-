# state：状态

环境信息。

# action：动作

智能体的运动。

# agent：智能体

# policy：策略

强化学习学什么，就是学的policy函数。

通过策略来控制智能体的运动。

根据观测状态s给出各种动作的可能性。

## 例子

![image-20230213上午93114533](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%8893114533.png)

# reward：奖励

执行一个action后获得的奖励值。

主要目标的奖励应该设置的很大，次要目标的奖励设置的相对较小。

## 例子

![image-20230213上午93702384](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%8893702384.png)

# state transition：状态转移

智能体从旧状态做出动作后变成新状态的过程称为状态转移。

状态转移可以是确定的，也可以是随机的。通常状态转移是随机的。

状态转移的随机性从环境中来。

## 例子

![image-20230213上午93948947](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%8893948947.png)

# return：累计回报

![image-20230213上午100710218](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%88100710218.png)

当前时刻的奖励Rt与未来时刻的奖励Rt+1相比，Rt更为重要，因为未来有很多不确定性。因此，未来的奖励所占的权重会逐渐降低。

在强化学习中一般使用折扣回报。

![image-20230213上午101011037](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%88101011037.png)

γ为0-1之间的数。γ的值会影响强化学习的学习效果。

==γ为超参数，需要自己调参。==

# Value Function：价值函数

## Action-Value Function

![image-20230213上午102327768](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%88102327768.png)

动作价值函数Qpi与当前状态St与动作at有关。

E [Ut]表示对Ut求期望（相当于求f（x，y）的期望，x、y可以是连续的，也可以是离散的）。

Ut未知，st和at为变量，且已知他俩的概率密度函数，我们则能用求期望的方法，表示Qpi而不需要求得Ut。

知道st和at，就知道下一时刻的奖励分布和状态转移分布，从而求期望。

![image-20230213上午102640475](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%88102640475.png)

对策略函数pi求最大化，取得最优动作价值函数。

- 动作价值函数Qpi，可以得到智能体处在状态St时做出动作at是否明智，给动作at打分。
- 强化学习也可以学习Q*。

## State-Value Function

![image-20230213上午103955217](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%88103955217.png)

![image-20230213上午104041737](%E5%90%8D%E8%AF%8D.assets/image-20230213%E4%B8%8A%E5%8D%88104041737.png)

- 当策略固定时，可以通过Vpi值的大小判断当前智能体所处的状态是好是坏。值越大状态越好，值越小状态越坏。
- Vpi的平均值可以用来判断策略的好坏，策略越好，Vpi的平均值越大。
