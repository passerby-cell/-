学习一个函数近似Q*。

# DQN

使用Q(s,a,w)近似Q*

![image-20230213下午34823439](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8834823439.png)

# Temporal Difference (TD) Learning( 常用于如何训练DQN)

![image-20230213下午35402407](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8835402407.png)

## 例子1

从纽约驾车去亚特兰大，预测花了1000分钟，实际上花了860分钟。由此计算损失函数，然后反向传播、梯度下降获取Wt+1的值。

α表示学习率。

- 缺点是运行完一整个流程才能对模型进行更新。

## 例子2

如何在中途进行训练，不用运行完一整个流程？

![image-20230213下午40104335](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8840104335.png)