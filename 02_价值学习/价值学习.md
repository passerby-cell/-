学习一个函数近似Q*。

# DQN

使用Q(s,a,w)近似Q*

![image-20230213下午34823439](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8834823439.png)

# Temporal Difference (TD) Learning( 常用于如何训练DQN)

![image-20230213下午35402407](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8835402407.png)

## 例子1

从纽约驾车去亚特兰大，预测花了1000分钟，实际上花了860分钟。由此计算损失函数，然后反向传播、梯度下降获取Wt+1的值。

α表示学习率或者步长。

- 缺点是运行完一整个流程才能对模型进行更新。

## 例子2

如何在中途进行训练，不用运行完一整个流程？

![image-20230213下午40104335](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8840104335.png)

从纽约开车去亚特兰大，途经华盛顿。预测值为1000分钟，但是，到达华盛顿时所花的真实时间是300分钟，预测从华盛顿到亚特兰大所花的时间为600分钟。

![image-20230214下午31545410](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8831545410.png)

因此，更新后的预测值为900分钟，更新后的预测值比最初的预测值更加准确。计算损失函数并反向传播，更新神经网络的权重。