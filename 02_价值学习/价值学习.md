学习一个函数近似Q*。

# DQN

使用Q(s,a,w)近似Q*

![image-20230213下午34823439](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8834823439.png)

# Temporal Difference (TD) Learning( 常用于如何训练DQN)

![image-20230213下午35402407](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8835402407.png)

## 例子1

从纽约驾车去亚特兰大，预测花了1000分钟，实际上花了860分钟。由此计算损失函数，然后反向传播、梯度下降获取Wt+1的值。

α表示学习率或者步长。

- 缺点是运行完一整个流程才能对模型进行更新。

## 例子2

如何在中途进行训练，不用运行完一整个流程？

![image-20230213下午40104335](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230213%E4%B8%8B%E5%8D%8840104335.png)

从纽约开车去亚特兰大，途经华盛顿。预测值为1000分钟，但是，到达华盛顿时所花的真实时间是300分钟，预测从华盛顿到亚特兰大所花的时间为600分钟。

![image-20230214下午31545410](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8831545410.png)

因此，更新后的预测值为900分钟，更新后的预测值比最初的预测值更加准确。计算损失函数并反向传播，更新神经网络的权重。

# 如何将TD算法应用于DQN？

![image-20230214下午32548134](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8832548134.png)

像上式等式左边有一项，等式右边有两项，左边为预测值，右边为真实值和预测值才可以使用TD算法。

Q(St,at,w)相当于从纽约开车到亚特兰大的预测时间，rt相当于从纽约开车到华盛顿的真实值，γ·Q(St+1,at+1,w)相当于从华盛顿开车到亚特拉大的预测值。

![image-20230214下午43800210](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8843800210.png)

![image-20230214下午44138375](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8844138375.png)

![image-20230214下午44241879](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8844241879.png)

![image-20230214下午44508776](%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0.assets/image-20230214%E4%B8%8B%E5%8D%8844508776.png)

 